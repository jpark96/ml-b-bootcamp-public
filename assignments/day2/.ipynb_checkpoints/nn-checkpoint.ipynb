{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying MNIST With Neural Networks Using Tensorflow\n",
    "\n",
    "In this assignment you will be training a neural network with a Tensorflow implementation of a simple two-layer fully connected neural network. Due to the complexity of constructing a neural network from scratch and that we are not testing you on the knowledge of specific neural network libraries, there will no coding for this assigment; instead, we will be asking relevant questions to test your understanding of how neural networks function. Make sure that you have Tensorflow installed in your environment before beginning this assignment.\n",
    "\n",
    "Let's begin by importing the necessary modules and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin the neural network by defining the input layer, which we will call `x`, and the correct labels, which we will call `label`. Since each image is 28 by 28 pixels, we need 784 input neurons. We are predicting between 10 digits, and so our labels will be expressed with a 10-dimensional vector using one-hot encoding (where all elements are 0 except for a single 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "label = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct our hidden layer with 200 neurons by creating a 784 by 400 matrix and a bias, and then apply a sigmoid activation function. We'll call the output of the hidden layer `h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_h = tf.Variable(tf.truncated_normal([784, 400], stddev=0.1))\n",
    "b_h = tf.Variable(tf.truncated_normal([400], stddev=0.1))\n",
    "h = tf.sigmoid(tf.matmul(x, W_h) + b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "What is the purpose of applying an activation function at each layer? How would our neural network be limited if we didn't apply any activation functions to the layers?\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "### Question 2:\n",
    "A problem with deep neural networks with sigmoid or tanh activation function is the vanishing gradient problem. Luckily, our neural network isn't deep enough for it be be an issue, but from observing the behavior of the sigmoid and tanh functions, can you see what problem might arise from computing gradients, especially in the first layers? What can be done to mitigate or resolve this?\n",
    "\n",
    "*YOUR ANSWER HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct the output layer. Since the neural network is predicting from 10 choices, we create a 400 by 10 matrix with bias, and then apply a softmax activation. The softmax function is like a generalized sigmoid, with the property that the sum of all outputs is equal to one (along with other properties that are similar to those from the sigmoid function). We will call our final output prediction `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_y = tf.Variable(tf.truncated_normal([400, 10], stddev=0.1))\n",
    "b_y = tf.Variable(tf.truncated_normal([10], stddev=0.1))\n",
    "y = tf.nn.softmax(tf.matmul(h, W_y) + b_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our loss function. For this neural network, we'll be using the cross entropy loss. Tensorflow also provides an operation that automatically finds the gradients of all operations in the neural network and performs the backpropagation step. We'll do backprop with the standard gradient descent optimizer that dynamically reduces the learning rate over time; we'll start with a learning rate of `0.2`.\n",
    "\n",
    "We'll also create an operation that lets us test the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(-tf.reduce_sum(label * tf.log(y), reduction_indices=[1]))\n",
    "backprop = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "correct = tf.equal(tf.argmax(y,1), tf.argmax(label,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "If you were to calculate the backpropagation step yourself, how would you do it? Explain in words the general method for performing the backpropagation step. Please be as detailed in your explanation as possible.\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "### Question 4:\n",
    "Why might it be helpful to reduce the learning rate (the coefficient multiplied to the gradient before subtracting it from the weights) over time?\n",
    "\n",
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next segment of code is for Tensorflow initialization; you don't need to worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've constructed the neural network, it time to start training. We'll train in mini-batches of 100 for 5000 steps. This may take a few minutes to execute. Training a neural network takes a lot of computing power!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5000):\n",
    "    x_batch, y_batch = data.train.next_batch(100)\n",
    "    sess.run(backprop, feed_dict={x: x_batch, label: y_batch})\n",
    "    if (i+1) % 500 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x: x_batch, label: y_batch})\n",
    "        print('Iteration {}, current training accuracy is {}'.format(i+1, train_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "What are the advantages and disadvantages of using mini-batches rather than pure stochastic training for each training step?\n",
    "\n",
    "*YOUR ANSWER HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our neural network performs on the test set. You should be getting around 95% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sess.run(accuracy, feed_dict={x: data.test.images, label: data.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've successfully built and trained the neural network, try playing around with adjusting some parameters! Some suggestions include: different batch sizes, mean squared error loss, changing the number of neurons in the hidden layer, changing the activation function, or adding more layers. If you're feeling especially daring, you can even try implementing a convolutional neural network, which can achieve more than 99% accuracy!\n",
    "\n",
    "This assignment was inspired by Google's MNIST for ML Beginners tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
